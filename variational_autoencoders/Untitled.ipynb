{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the idea and execution below belong to https://towardsdatascience.com/variational-autoencoders-explained-in-detail-d585327c660a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Datasets' object has no attribute 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-263a1d57bc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Datasets' object has no attribute 'load_data'"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = x_train[0].shape[0] * x_train[0].shape[1]\n",
    "num_digits = len(set(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Url parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'encoder_layers': [128],                # the encoder will be implemented using a simple feed forward network\n",
    "    'decoder_layers': [128],                # and so will the decoder (CNN will be better, but I want to keep the code simple)\n",
    "    'digit_classification_layers': [128],   # this is for the conditioning. I'll explain it later on\n",
    "    'activation': tf.nn.sigmoid,            # the activation function used by all sub-networks\n",
    "    'decoder_std': 0.5,                     # the standard deviation of P(x|z) discussed in the first post\n",
    "    'z_dim': 10,                            # the dimension of the latent space\n",
    "    'digit_classification_weight': 10.0,    # this is for the conditioning. I'll explain it later on\n",
    "    'epochs': 20,\n",
    "    'batch_size': 100,\n",
    "    'learning_rate': 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Url model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  General idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is composed of three sub-networks:\n",
    "\n",
    "1. Given x (image), encode it into a distribution over latent space, referred as $Q(z|x)$ \n",
    "2. Given z in latent space (code representation of an image), decode it into the image it represents - referred to as $f(z)$ \n",
    "3. Givent x, classify its digit by mapping it to a layer of size 10 where the $i^{th}$ value contains the probability of the $i^{th}$ digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 sub-networks are the main idea of the variational autoencoders framework. In addition, they also include the third sub-network, whose main purpose is to enforce some of the information store in the latent space onto the digit found in the image (?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, layers):\n",
    "    for layer in layers:\n",
    "        x = tf.layers.dense(x, layer, activation=params['activation'])\n",
    "    mu = tf.layers.dense(x, params['z_dim'])\n",
    "    var = 1e-5 + tf.exp(tf.layers.dense(x, params['z_dim']))\n",
    "    return mu, var\n",
    "def decoder(z, layers):\n",
    "    for layer in layers:\n",
    "        z = tf.layers.dense(z, layer, activation=params['activation'])\n",
    "    mu = tf.layers.dense(z, input_size)\n",
    "    return tf.nn.sigmoid(mu)\n",
    "def digit_classifier(x, layers):\n",
    "    for layer in layers:\n",
    "        x = tf.layers.dense(x, layer, activation=params['activation'])\n",
    "    logits = tf.layers.dense(x, num_digits)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = tf.placeholder(tf.float32, [None, input_size])\n",
    "digits = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# encode an image into a distribution over the latent space\n",
    "encoder_mu, encoder_var = encoder(images,\n",
    "                                  params['encoder_layers'])\n",
    "\n",
    "# sample a latent vector from the latent space - using the reparameterization trick\n",
    "eps = tf.random_normal(shape=[tf.shape(images)[0],\n",
    "                              params['z_dim']],\n",
    "                       mean=0.0,\n",
    "                       stddev=1.0)\n",
    "z = encoder_mu + tf.sqrt(encoder_var) * eps\n",
    "\n",
    "# classify the digit\n",
    "digit_logits = digit_classifier(images,\n",
    "                                params['digit_classification_layers'])\n",
    "digit_prob = tf.nn.softmax(digit_logits)\n",
    "\n",
    "# decode the latent vector - concatenated to the digits classification - into an image\n",
    "decoded_images = decoder(tf.concat([z, digit_prob], axis=1),\n",
    "params['decoder_layers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss is composed of how well we can reconstruct the image\n",
    "loss_reconstruction = -tf.reduce_sum(\n",
    "    tf.contrib.distributions.Normal(\n",
    "        decoded_images,\n",
    "        params['decoder_std']\n",
    "    ).log_prob(images),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# and how off the distribution over the latent space is from the prior.\n",
    "# Given the prior is a standard Gaussian and the inferred distribution\n",
    "# is a Gaussian with a diagonal covariance matrix, the KL-divergence\n",
    "# becomes analytically solvable, and we get\n",
    "loss_prior = -0.5 * tf.reduce_sum(\n",
    "    1 + tf.log(encoder_var) - encoder_mu ** 2 - encoder_var,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "loss_auto_encode = tf.reduce_mean(\n",
    "    loss_reconstruction + loss_prior,\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "# digit_classification_weight is used to weight between the two losses,\n",
    "# since there's a tension between them\n",
    "loss_digit_classifier = params['digit_classification_weight'] * tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=digits,\n",
    "                                                   logits=digit_logits),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "loss = loss_auto_encode + loss_digit_classifier\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(params['learning_rate']).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "losses_auto_encode = []\n",
    "losses_digit_classifier = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(params['epochs']):\n",
    "        for _ in range(int(mnist.train.num_examples / params['batch_size'])):\n",
    "            batch_images, batch_digits = mnist.train.next_batch(params['batch_size'])\n",
    "            sess.run(train_op, feed_dict={images: batch_images, digits: batch_digits})\n",
    "        \n",
    "        train_loss_auto_encode, train_loss_digit_classifier = sess.run(\n",
    "            [loss_auto_encode, loss_digit_classifier],\n",
    "            {images: mnist.train.images, digits: mnist.train.labels})\n",
    "        \n",
    "        losses_auto_encode.append(train_loss_auto_encode)\n",
    "        losses_digit_classifier.append(train_loss_digit_classifier)\n",
    "        \n",
    "        sample_z = np.tile(np.random.randn(1, params['z_dim']), reps=[num_digits, 1])\n",
    "        gen_samples = sess.run(decoded_images,\n",
    "                               feed_dict={z: sample_z, digit_prob: np.eye(num_digits)})\n",
    "        samples.append(gen_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.train.num_examples / params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.plot(losses_auto_encode)\n",
    "plt.title('VAE loss')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(losses_digit_classifier)\n",
    "plt.title('digit classifier loss')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples):\n",
    "    IMAGE_WIDTH = 0.7\n",
    "    plt.figure(figsize=(IMAGE_WIDTH * num_digits,\n",
    "                        len(samples) * IMAGE_WIDTH))\n",
    "    for epoch, images in enumerate(samples):\n",
    "        for digit, image in enumerate(images):\n",
    "            plt.subplot(len(samples),\n",
    "                        num_digits,\n",
    "                        epoch * num_digits + digit + 1)\n",
    "            plt.imshow(image.reshape((28, 28)),\n",
    "                       cmap='Greys_r')\n",
    "            plt.gca().xaxis.set_visible(False)\n",
    "            if digit == 0:\n",
    "                plt.gca().yaxis.set_ticks([])\n",
    "                plt.ylabel('epoch {}'.format(epoch + 1),\n",
    "                           verticalalignment='center',\n",
    "                           horizontalalignment='right',\n",
    "                           rotation=0,\n",
    "                           fontsize=14)\n",
    "            else:\n",
    "                plt.gca().yaxis.set_visible(False)\n",
    "                \n",
    "                \n",
    "plot_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
